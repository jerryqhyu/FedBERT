{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train_fedbert.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"_VEAbGqVletS","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Check availble memory of GPU\n","# Check that we are using 100% of GPU\n","# memory footprint support libraries/code\n","!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n","!pip -q install gputil\n","!pip -q install psutil\n","!pip -q install humanize\n","import psutil\n","import humanize\n","import os\n","import GPUtil as GPU\n","GPUs = GPU.getGPUs()\n","# XXX: only one GPU on Colab and isnâ€™t guaranteed\n","gpu = GPUs[0]\n","def printm():\n"," process = psutil.Process(os.getpid())\n"," print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n"," print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n","printm()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KbjCaEdolTzw","colab_type":"code","colab":{}},"source":["!pip install -qq transformers\n","!pip install -qq nlp"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eu9NHxTvdZxO","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_AUMNVmjluPo","colab_type":"code","colab":{}},"source":["# imports\n","from transformers import (\n","    BertForMaskedLM,\n","    BertForNextSentencePrediction,\n","    BertModel,\n","    BertTokenizer,\n","    BertConfig,\n","    Trainer,\n","    BertForPreTraining,\n","    DataCollatorForLanguageModeling,\n","    DataCollatorForNextSentencePrediction,\n","    TrainingArguments,\n","    LineByLineTextDataset,\n","    TextDatasetForNextSentencePrediction,\n",")\n","import torch\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from torch.utils.data.dataset import Dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e0BoKyBXsYdX","colab_type":"code","colab":{}},"source":["pretrain = 'BERT'\n","task = 'MLM'\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","\n","if task == 'MLM':\n","    dataset = LineByLineTextDataset(\n","        tokenizer=tokenizer,\n","        file_path=\"/content/drive/My Drive/230T2 MLTS/Colab Notebooks/data/new_sifted_Speech.txt\",\n","        block_size=128,\n","    )\n","    eval_dataset = LineByLineTextDataset(\n","        tokenizer=tokenizer,\n","        file_path=\"/content/drive/My Drive/230T2 MLTS/Colab Notebooks/data/new_sifted_Statements.txt\",\n","        block_size=128,\n","    )\n","    bert_type = BertForMaskedLM\n","    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n","elif task == 'NSP':\n","    dataset = TextDatasetForNextSentencePrediction(\n","        tokenizer=tokenizer,\n","        file_path=\"/content/drive/My Drive/230T2 MLTS/Colab Notebooks/data/new_sifted_Speech.txt\",\n","        block_size=32,\n","    )\n","    bert_type = BertForPreTraining\n","    data_collator = DataCollatorForNextSentencePrediction(tokenizer=tokenizer, mlm=True, block_size=32)\n","else:\n","    bert_type = BertModel\n","\n","if pretrain == 'BERT':\n","    model = bert_type.from_pretrained('/content/drive/My Drive/230T2 MLTS/Colab Notebooks/params/BERT/checkpoint-100000')\n","elif pretrain =='FIN':\n","    model = bert_type.from_pretrained('/content/drive/My Drive/230T2 MLTS/Colab Notebooks/params/FIN/checkpoint-100000')\n","else:\n","    model = bert_type.from_pretrained('/content/drive/My Drive/230T2 MLTS/Colab Notebooks/params/PRIME/checkpoint-100000')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EtS-X1I5cHEm","colab_type":"text"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"ffShBXVisaW3","colab_type":"code","colab":{}},"source":["training_args = {\n","    \"output_dir\": \"/content/drive/My Drive/230T2 MLTS/Colab Notebooks/params/\"+pretrain,\n","    \"overwrite_output_dir\": True,\n","    \"logging_dir\": \"/content/drive/My Drive/230T2 MLTS/Colab Notebooks/params/logs\",\n","    \"learning_rate\": 1e-4,\n","    \"do_train\": True,\n","    \"do_eval\": True,\n","    \"max_steps\": 50000,\n","    \"warmup_steps\": 100,\n","    \"weight_decay\": 0.001,\n","    \"per_device_train_batch_size\": 32,\n","    \"per_device_eval_batch_size\": 4,\n","    \"logging_steps\": 2500,\n","    \"fp16\": True,\n","    \"save_steps\": 5000,\n","    \"save_total_limit\": 2,\n","}\n","\n","training_args = TrainingArguments(**training_args)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UiTC7EZkvOZV","colab_type":"code","colab":{}},"source":["model = model.train()\n","\n","# create the trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=dataset\n",")\n","# train\n","trainer.train()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kdThAf_SwEgH","colab_type":"text"},"source":["## Eval"]},{"cell_type":"code","metadata":{"id":"IrMHENX7zd0H","colab_type":"code","colab":{}},"source":["import random\n","models = [BertForMaskedLM.from_pretrained('/content/drive/My Drive/230T2 MLTS/Colab Notebooks/params/BERT/checkpoint-100000'),\n","        BertForMaskedLM.from_pretrained('/content/drive/My Drive/230T2 MLTS/Colab Notebooks/params/FIN/checkpoint-100000'),\n","        BertForMaskedLM.from_pretrained('/content/drive/My Drive/230T2 MLTS/Colab Notebooks/params/PRIME/checkpoint-100000'),\n","        BertForMaskedLM.from_pretrained('/content/drive/My Drive/230T2 MLTS/Colab Notebooks/params/FinBERT-Prime_128MSL-250K'),\n","        BertForMaskedLM.from_pretrained('bert-base-uncased')\n","]\n","\n","dataset = LineByLineTextDataset(\n","        tokenizer=tokenizer,\n","        file_path=\"/content/drive/My Drive/230T2 MLTS/Colab Notebooks/data/new_sifted_Speech.txt\",\n","        block_size=128,\n",")\n","\n","eval_dataset = LineByLineTextDataset(\n","    tokenizer=tokenizer,\n","    file_path=\"/content/drive/My Drive/230T2 MLTS/Colab Notebooks/data/new_sifted_Statements.txt\",\n","    block_size=128,\n",")\n","\n","model_names = ['FedBERT-BERT', 'FedBERT-Fin', 'FedBERT-Prm', 'FinBERT', 'BERT']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i4zrmUaE2PPH","colab_type":"code","colab":{}},"source":["probs = [0.05, 0.15, 0.25, 0.35, 0.45]\n","train_loss = [[] for i in range(len(models))]\n","eval_loss = [[] for i in range(len(models))]\n","\n","n = 250\n","train_idxes = random.sample(list(range(len(dataset))), n)\n","eval_idxes = random.sample(list(range(len(eval_dataset))), n)\n","for prob in probs:\n","    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=prob)\n","    for m, model in enumerate(models):\n","        total_loss = 0\n","        for i in train_idxes:\n","            outputs = model(data_collator.mask_tokens(dataset[i].unsqueeze(0))[0], labels=dataset[i].unsqueeze(0))\n","            total_loss += outputs[0].item()\n","        train_loss[m].append(total_loss/n)\n","\n","        total_loss = 0\n","        for i in eval_idxes:\n","            outputs = model(data_collator.mask_tokens(eval_dataset[i].unsqueeze(0))[0], labels=eval_dataset[i].unsqueeze(0))\n","            total_loss += outputs[0].item()\n","        eval_loss[m].append(total_loss/n)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OEcgRZZKIMYW","colab_type":"code","colab":{}},"source":["fig, ax = plt.subplots(ncols=2, figsize=(12,6), constrained_layout=True, sharey=True)\n","for i in range(len(train_loss)):\n","    ax[0].plot(probs, train_loss[i], label=model_names[i])\n","for i in range(len(train_loss)):\n","    ax[1].plot(probs, eval_loss[i], label=model_names[i])\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AvWHs9_SryIy","colab_type":"code","colab":{}},"source":["plt.figure(figsize=(12,4))\n","for i in range(len(train_loss)):\n","    plt.plot(probs, np.array(train_loss[i]) - np.array(eval_loss[i]), label=model_names[i])\n","plt.axhline(y=0, color='black', linestyle='-.')\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oR7WiwAucSPc","colab_type":"text"},"source":["## Using the models"]},{"cell_type":"code","metadata":{"id":"ebiofsHXe4QB","colab_type":"code","colab":{}},"source":["from sklearn.metrics.pairwise import cosine_similarity"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z76rmET5cTGl","colab_type":"code","colab":{}},"source":["# s1 = 'the existence of substantial economic shock'\n","# s2 = 'the man being hit by the car is in shock'\n","\n","# s1 = 'The man was accused of robbing a bank'\n","# s2 = 'The man went fishing by the river bank'\n","\n","s1 = 'the fed is promoting its long term price stability target'\n","s2 = 'it is the most purchased item at his local target'\n","\n","token_1 = torch.tensor(tokenizer.encode(s1)).unsqueeze(0)\n","token_2 = torch.tensor(tokenizer.encode(s2)).unsqueeze(0)\n","\n","for m, model in enumerate(models):\n","    embedding_1 = model(token_1)\n","    embedding_2 = model(token_2)\n","    last_hidden_states_1 = embedding_1[0][0].detach().numpy()\n","    last_hidden_states_2 = embedding_2[0][0].detach().numpy()\n","    print(model_names[m], cosine_similarity(last_hidden_states_1[[-2]], last_hidden_states_2[[-2]]).round(2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p4QJ7IDAldHJ","colab_type":"code","colab":{}},"source":["sentence = 'the federal reserve is committed to using its full range of tools to support the u.s. economy in this challenging time, thereby promoting its maximum employment and price stability goals'\n","fed_tokenized = tokenizer.tokenize(sentence)\n","fed_mask_idx = [11, 19, 26, 29, 32]\n","print([fed_tokenized[i] for i in fed_mask_idx])\n","\n","# sentence = 'And, while many of these changes have improved the efficiency of our financial system and lowered costs for consumers, it is only realistic to1 acknowledge that they also present new and sometimes daunting tests for community banks'\n","# fed_tokenized = tokenizer.tokenize(sentence)\n","# fed_mask_idx = [13, 17, 19, 38, 41]\n","# print([fed_tokenized[i] for i in fed_mask_idx])\n","\n","for i in fed_mask_idx:\n","    fed_tokenized[i] = '[MASK]'\n","\n","fed_tokens_tensor = torch.tensor([tokenizer.convert_tokens_to_ids(fed_tokenized)])\n","\n","preds  = {}\n","for m, model in enumerate(models):\n","    with torch.no_grad():\n","        preds[model_names[m]] = model(fed_tokens_tensor)[0]\n","        tokens = []\n","        for i in fed_mask_idx:\n","            predicted_index = torch.argmax(preds[model_names[m]][0, i]).item()\n","            predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n","            tokens.append(predicted_token)\n","        print(f'{model_names[m]} : {tokens}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0OY4URNMxJO5","colab_type":"code","colab":{}},"source":["sentence = 'the federal reserve is committed to using its full range of tools to support the u.s. economy in this challenging time, thereby promoting its maximum employment and price stability goals'\n","fed_tokenized = tokenizer.tokenize(sentence)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AX7ZxKXJxOjQ","colab_type":"code","colab":{}},"source":["sentence = ' And, while many of these changes have improved the efficiency of our financial system and lowered costs for consumers, it is only realistic to1 acknowledge that they also present new and sometimes daunting tests for community banks'\n","fed_tokenized = tokenizer.tokenize(sentence)\n","fed_mask_idx = [13, 17, 19, 38, 41]\n","print([fed_tokenized[i] for i in fed_mask_idx])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WAm3xA5LxP3S","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}